#
# ORCHESTRATOR VSPHERE BASE CONFIGURATION
#
# Document last updated: Aug 27, 2015
#
# This configuration file is meant to be used to provision clusters on
# the vSphere setup within the office. It has defaults set up for connecting to
# vCenter and specifying the hosts to create machines on.
#
# NOTE: IP addresses may need to be changed and mapped to avoid overlapping if
# multiple people are playing around with the Orchestrator at the same time.
#


# The provisioner specifies information related to the creation of the
# machines that will run within the cluster.
provisioner {
  type: vsphere

  # Credentials to connect to vSphere Server
  hostname: "172.27.0.6"
  username: "administrator@vsphere.local"
  password: "*BRrQzf6M1"

  # Insecure is needed when vCenter is using a self-signed certificate.
  insecure: true

  # The name of the datacenter within vCenter. This is generally optional, but
  # may be needed to help the orchestrator locate specific hosts/clusters or
  # resource pools.
  datacenter: "MWdc1"

  # vm_folder sets the folder within the vCenter datacenter where the machines
  # should be created.
  vm_folder: "Scott"

  # template_folder sets the folder within vCenter that the base template image
  # is located.
  # template_folder: "continuum"

  # Define the compute resources that will be used within vCenter to run
  # across. These are the names of the ESXi hosts listed under
  # "hosts and clusters" in vCenter. You may also specify the name of a
  # vSphere cluster. You may not specify ESXi hosts which are members of
  # a vSphere cluster.
  compute_resources: [
    "172.27.0.13",
    "172.27.0.14",
    "172.27.0.15"
  ]

  # Resourse pools are only suppoered on non-clustered ESXi hosts at this time
  #resource_pools: [
  #  "Apcera"
  #]

  # A list of datastores which which may be used. The datastore with the
  # least free space is selected by default
  #datastores: [
  #   "san_datastore"
  #]

  # Define default settings for machines. Settings within the machines section
  # will inherit or overwrite these defaults.
  machine_defaults: {
    # The format to name machines
    name_format: "taftan-$config-$name"
    dns:    ["8.8.8.8", "8.8.4.4"]

    # This setting is required on systems which are slow to clone VMs
    #boot_timeout: "30m"

    # Name must match a VM Portgroup. This Portgorup should exist on all
    # of the compute_resouces. Static IP pools managed by orchestrator
    # or DHCP are suppoerted.
    networks: [
      {
        name: "lab"
        mode: "dhcp"
        #ip_range: "10.1.0.100-199"
        #subnet: "255.255.255.0"
        #gateway: "10.1.0.1"
      }
    ]
  }
}

# Define the machines types that will be used within the cluster. These serve as
# a template to define the properties of the machines that will be created
# within vSphere.  This also includes the tags that would be allowed on the
# machine type. The tags map to Chef tags that will be placed on the
# systems. The deploy specifies a mapping of tags to Chef roles that should be
# run on each system.
machines: {
  # auditlog database
  audit: {
    cpu:    2
    memory: 4096
    disks: [
        { size: 50, purpose: auditlog }
    ]
    suitable_tags: [ "auditlog-database" ]
  }

  monitoring: {
    cpu:    2
    memory: 4096

    # Override the default network to use static IP via Orchestrator. The IP is
    # known in advance for firewall and load balamcer configuration.  Each
    # static IP allocation requires 'component_count * 2' IPs to support
    # reprovisioning and upgrades. At this time, configuration updates to
    # firewall and load balamcer devices must be performed manually.

    # Override the default datastore. One may wish to do this to detect
    # failures in central sotrage
    #datastores: [
    # "san_monitoring"
    #]

    suitable_tags: [
      "monitoring"
    ]

  }

  # (HTTP) Routers handle all incoming HTTP/HTTPS traffic.
  router: {
    cpu:    2
    memory: 4096

    # Override the default network to use static IP via Orchestrator. The IP is
    # known in advance for firewall and load balamcer configuration.  Each
    # static IP allocation requires 'component_count * 2' IPs to support
    # reprovisioning and upgrades. At this time, configuration updates to
    # firewall and load balamcer devices must be performed manually.

    suitable_tags: [
      "router"
    ]
  }

  # The central machine is used to run the various common components that are
  # n-wise scalable.
  central: {
    cpu:    4
    memory: 4096
    disks: [
      { size: 100, purpose: package-storage }
    ]
    #networks: [
    #{
    #  name: "lab"
    #  ip_range: "172.27.17.54-55"
    #  subnet: "255.255.248.0"
    #  gateway: "172.27.16.1"
    #}
    #]
    suitable_tags: [
      "component-database"
      "api-server"
      "job-manager"
      "cluster-monitor"
      "stagehand"
      "package-manager"
    ]
  }

  # The singleton box is currently used to run the components that are currently
  # limited to only have one active at a time.
  singleton: {
    cpu:    4
    memory: 4096
    suitable_tags: [
      "nats-server"
      "auth-server"
      "health-manager"
      "metrics-manager"
    ]
  }

  # The instance_manager boxes run the Instance Managers within the
  # cluster. They are where the job workloads are executed and are the machines
  # that are generally scaled up the most within a cluster. Please review the
  # vSphere installation requirments for more info.
  instance_manager: {
    # In general, vcpu should be set to 'physical_core_count_per_socket - 2'
    cpu:   4
    # In general, vram should be set to '(physical_ram / 2) * .95'
    memory: 32768
    disks: [
      { size: 100, purpose: instance-manager }
    ]
    suitable_tags: [ "instance-manager" ]
  }

  # Metrics database (Graphite) for jobs and components.
  metrics: {
    cpu:    2
    memory: 4096
    disks: [
       { size: 100, purpose: graphite }
    ]
    suitable_tags: [ "graphite-server" ]
  }

  # Circular box is used to run Redis to provide a circular buffer for recent
  # log lines from jobs running within the cluster.
  job_logs: {
    cpu:    1
    memory: 2048
    disks: [
       { size: 100, purpose: redis-server }
    ]
    suitable_tags: [ "redis-server" ]
  }

  # The IP Manager is on a dedicated host so that it has its own Public IP. It
  # is used for the Fixed IP service / IP Manager and allows jobs to have their
  # source IP when connecting outbound come from a single consistent
  # source. This is generally intended for legacy applications or services
  # outside the cluster that rely on IP whitelisting. This machine and
  # functionality is optional.
  ip_manager: {
    cpu:    1
    memory: 2048

    # Override the default network to use static IP via Orchestrator. The IP is
    # known in advance for firewall and load balamcer configuration.  Each
    # static IP allocation requires 'component_count * 2' IPs to support
    # reprovisioning and upgrades. At this time, configuration updates to
    # firewall and load balamcer devices must be performed manually.

    suitable_tags: [
      "ip-manager"
    ]
  }

  # TCP Router is on a dedicated host so that it has it own dedicated public
  # IP. It is used for handling general TCP routes, simmilar to the HTTP
  # router. It is optional within the cluster, but will be necessary to be able
  # to use any tcp or non-http routes on jobs.
  tcp_router: {
    cpu:    1
    memory: 2048

    # Override the default network to use static IP via Orchestrator. The IP is
    # known in advance for firewall and load balamcer configuration.  Each
    # static IP allocation requires 'component_count * 2' IPs to support
    # reprovisioning and upgrades. At this time, configuration updates to
    # firewall and load balamcer devices must be performed manually.
    suitable_tags: [
      "tcp-router"
    ]
  }

  # S3 Compatible object store for packages. Depends on the router component
  # to load balance requests. Cluster nodes must be able to resolve
  # packages.s3.$base_domain to one or more Continnum routers via DNS during
  # the cluster deployment.
  object_storage: {
    cpu: 2
    memory: 16384
    disk: [
      { size: 100, purpose: riak }
    ]
    suitable_tags: [
      "riak-node"
    ]
  }
}

# The components section specifies the desired number of each of the component
# types. Changes here will either find a new place to run components or scale
# the cluster down if the numbers are decreased.
components: {
  # Monitoring
          monitoring: 0

  # Central Components
  component-database: 2
          api-server: 2
         job-manager: 2
     package-manager: 2
     cluster-monitor: 1

  # Two routers for use with a load balancer.
              router: 2

  # Stagehand handles loading various dependencies into the cluster, once it is
  # up and ready. These include the default stagers, web console, documentation,
  # and ensures default services are registered within the cluster.
           stagehand: 1

  # Instance Managers run workload. In general, set this value to:
  # (cpu_count_of_all_compute_resources * resource allocation for continuum) - 2
    instance-manager: 3

  # Singleton Apcera HCOS Components
         auth-server: 1
      health-manager: 1
     metrics-manager: 1
         nats-server: 1
     graphite-server: 1
        redis-server: 1

  # Object storage for packages
           riak-node: 3

  # auditlog databases
   auditlog-database: 2

  # Singletons within the system that are role specific. These are porocesses
  # that only need one of in the cluster. These roles witihn the cluster are
  # generally optional.
          tcp-router: 1
          ip-manager: 0
}

# Settings that will get pulled into Chef and made available to machines within
# the cluster.
chef: {

  # site specific NTP Servers
  "ntp": {
    # The default is 4 pool members of the public time pool.
    #"servers": [
    #  "10.0.0.100",
    #  "10.0.0.101",
    #  "10.0.0.102",
    #  "10.0.0.103",
    #]
  },

  # Uncomment and configure to define a site specific mail relay
  "postfix": {
    #"main": {
      # "relayhost": "10.0.0.50"
    #}
  }

  "apzabbix": {
    "db": {
      # location of the Postgres Database for monitoring.
      #  setting to localhost:5432 will install Postgres  on
      #  the monitoring server
      "hostport": "localhost:5432",

      # Monitoring database master user
      "master_user": "apcera_ops",

      # Monitoring database master password - please supply your own password
      "master_pass": "Generate A Password for here",

      # Monitoring database user name and password - please supply your own password
      "zdb_user": "zabbix",
      "zdb_pass": "Generate A Password for here"
    },

    # Logins for the monitoring system
    "users": {
      "guest": { "user": "monitoring", "pass": "Generate A Password for here" },
      # The system install insists upon a user called admin, so an entry for this user needs to exist
      "admin": { "user": "Admin", "pass": "Generate A Password for here", "delete": false }
    },

    # Hostnames monitoring will respond to
    "web_hostnames": ["cloud-bastion", "cloud.monitoring.example.com"]

    # Defined as a PD integration of type 'Zabbix'
    "pagerduty": {
      # Your Pager Duty API key
      #"key": "Your PD API Key Here",
       # During initial setup, only send pagerduty alerts on this cluster during these hours on weekdays
      #"period": "1-5,15:00-24:00"
    },
    "email": {
      # external-monitoring@apcera.com should only be used if Apcera is able to log into the cluster and administer it.
      # You may also use your own address to send alerts to
      "sendto": "external-monitoring@apcera.com",
      "smtp_server": "localhost"
      "smtp_helo": "localhost",
      # This is the "from" address
      #"smtp_email": "continuum-monitoring@example.net"
      "smtp_email": "continuum-monitoring@taftan.buffalo.im"
    }
  },

  "continuum": {

    # A name to identify the cluster. Used in hostnames
    # and alerting. Default is "Continuum"
    #"cluster_name": "cloud.example.net",
    "cluster_name": "taftan.buffalo.im",


    # DNS Name of the cluster.
    #"base_domain": "cloud.example.net",
    "base_domain": "taftan.buffalo.im",

    # List of URLs that will come to the Auth Server for authentication. These
    # should include the "console" and "auth" subdomains and currently needs to
    # list both "http" and "https".
    "portal_urls": [
      "http://console.taftan.buffalo.im",
      "https://console.taftan.buffalo.im",
      "http://auth.taftan.buffalo.im",
      "https://auth.taftan.buffalo.im"
    ],

    # Set the cluster's subnet. This is primarily needed for some understanding
    # of "internal" vs "external".
    "cluster": {
      "subnet": "172.27.0.0/24"
    },

    # Add any SSH keys that should be placed on machines provisioned within the
    # cluster. Each key should be a string entry in the array. The SSH keys will
    # be placed on the host by Chef and allow the system to be accessible by the
    # "ops" user, or using the "orchestrator-cli ssh" command. Any changes will
    # be applied during initial step of the Deploy action.
    "ssh": {
      "custom_keys":[
      # Name and contanct in for this key here
      "ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDcjVF+qO3Nu+Lx9MKYRPbD/mTXUgkeH54jfqVXIir5KPTh9kd1nKXVxrK7YH2CTc/2JHf03/FiFYnNUuEMnMxdOyd84qBNxfI0UUm2I5Zpik9fcthnDBk395Uqmhzw/XYSuQyN6vJ6r/3UjhM7MVVGid6nkF8ZderlC6jWo39XnWz2EwP9W26lFxcZm4sGqLfKxLx/Q/MAM4ZZhtokEAzGXPZgehi6z/pYConkMcbvBU8VFYcIKKP88et59krMcj9byEVZxl9eFk5H3ipkIR/nGqDqCm5EPaWQGKBjmzEzcvEoDQ9Etp8K54q06sQG+SrghniGCwwZ8fD1wsd44eBF ben@apcera.com"
      "ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAACAQCtvs2eXvF6kbQKMFUCQ5Mdd+HCXXZ/E+bxOoD1BvZNv7pEZe/xeR5pVMBZ6QWyXwsSa5EJgdUg73hNVl833zUn/n+boBgS6GJPRfuyIRSyUAK2zbZaDvsW5b+Uf+oC3uWyO9bgwqKl0bps5EShroyyk7p5ZCehRkvETfRC6gtr+4UqHN22cjSBImT04G4gV6LKFO/w1JJQqlsIqWrlv0uCtTxEHIk/mVmKWUJERbfK49z8pwpkXX8BU1GVFrCC+SmE39Z4/6XPsjH5XZdGMmeuGzBCZWD8ILhcHrHVncWjfvpxzYb8hfwBK5DmNxjS/+PFi0bR3B1//JlzH2qRRtpHoACjOxVgzwKNtWUpBr8W18BIf5OFKIthzrD74AS5Z6G2YSuVVABnDVFFQ2sV6KXPemr62cnpi+oXpyTo0V18w2xMYlIJm1tA4D+nk7SxLJ6mJH33t7TUgGQdd7v0C0rKd4+oQVoo6QQ1T8x4Xcvy9nNLIeCZKM1W27x+dqUgLcBz74jS0Eg6pezWkkn0EPGNRWQoxZn86/AW0WBG3rNxhgZNwDPYEEthx+bh1zpJ1jxvkHPR9jfwyRc/jCzY9eigXxO5gUwuPWcw6WIvSC0cTNwPgYJ/qMMuHhYhv6ryoPm7EvJqb/iwte7xZ6CENmXSi/T3uTUaoR+ryxucM7P/iw== scott.williamson@apcera.com"
      ]
    },

    # Auth Server settings. These set up the initital users that are imported
    # and given permission to the cluster. Once the cluster has been deployed,
    # any changes to these settings will not be applied. They're only used on
    # the initial boostrapping of cluster policy.
    "auth_server": {
      "identity": {
        "google": {
          "users": [
            "ben@apcera.com",
	    "scott.williamson@apcera.com"
          ],

          # These are the split credentials to isolate google auth for this cluster
          # First the google device flow for apc:
          # Google Project: "Description of gauth project name here"
          #"client_id": "Your client appid here.apps.googleusercontent.com"
          #"client_secret": "Your client secret here"
          # Then the id for the web flow
          #"web_client_id": "Your web client appid here.apps.googleusercontent.com"

          "client_id": "1005526534732-hgin6q3aj6p7n7q72u09vg2c7jp57j32.apps.googleusercontent.com"
          "client_secret": "cIAApNjsS9Qhfo6Gv4LtuVOv"
          "web_client_id": "1005526534732-7mdqi9ovnlo1rqp5u2dj5thnmtbbkfhd.apps.googleusercontent.com"

          # Google Auth Client created a different client secret for the web client ID
          # Will this work?
          #"web_client_secret": "hdaZZhxTsHz5vpq6GDdaezqi"

          # These oauth2 parameters will be used to generate a config
          # file for the oauth2 monitoring plugin. cluster must be
          # operation before configuriung this section
          #"scope_url": "http://could.company.com",
          #"refresh_token": "Monitoring refresh token"
        },
      },

      # Users added as admins on first deploy
      "admins": [
        "ben@apcera.com",
        "scott.williamson@apcera.com"
      ]
    },

    # Router settings. This is to configure the SSL certificate to apply to the
    # site.
    "router": {
      "ssl": {
        # Please refer to Configuring Apcera HCOS for HTTPS (SSL/TLS)
        "enable": false
      }
    },
  }
}
